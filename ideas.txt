pickle the embeddings one time, only words appear in the corpus
keep only words appear in the train/dev/test
and store it in pikcle file

// corpus, keep only sentence1 and sentence2

// preproecess the embeddings
bound = math.sqrt(3.0 / emb_size)
self.embedder.weight.data.uniform_(-1.0 * bound, bound)

xavier
MLP: layer hidden to hidden
softmax


train

unknown words - preprocess the vocab with words appear at least in one of the embeddings
drop data with label = '-'